common:
  save_interval: 1
  test_interval: 1
  log_interval: 5
  max_epoch: 20
  seed: 79911092
  amp: False  # keep it False!
  gpus: "0"

datasets:
  path: /PATH/TO/emg2qwerty-pretrain/
  batch_size: 256
  num_workers: 8
  pin_memory: True

checkpoint:
  resume: False
  checkpoint_path: ''
  save_folder: 'ckpt'
  save_location: '${checkpoint.save_folder}/emg_bs${datasets.batch_size}_' 

optimization:
  lr: 5e-4
  warmup: 5
  weights:
    l_t: 0.2
    l_f: 1.0

model:
  sample_rate: 1000
  normalize: False
  filters: 32
  ratios: [3, 3, 2]
  causal: True
  norm: weight_norm
  segment: None
  name: my_biocodec
  q_bins: 256
  n_q: 6

distributed:
  world_size: 1
  data_parallel: False
  find_unused_parameters: False
  torch_distributed_debug: False
  init_method: tcp
  master_addr: localhost
  master_port: 6008

hydra:
  run:
    dir: /PATH/TO/biocodec-runs/emg_${now:%Y-%m-%d}_${now:%H-%M-%S}
